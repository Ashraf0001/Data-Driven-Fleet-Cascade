# Fleet Decision Platform - Cursor Rules

## Project Context

This is an enterprise-grade decision intelligence platform for fleet operations that combines:
- Demand forecasting (multi-location time-series)
- Cascading optimization (min-cost flow + MILP)
- Contract intelligence (NLP/Vision extraction)
- Risk & survival prediction
- Explainability at every stage

**Architecture**: Modular, config-driven, MLOps-ready pipeline
**Tech Stack**: Python 3.9+, XGBoost, OR-Tools/PuLP, FastAPI, Pandas, Parquet
**Approach**: MVP-first, then progressive enhancement

## Development Tools & Environment

### Package Management
- **Use `uv`** for all Python package management (faster alternative to pip)
- Create `pyproject.toml` for project metadata and dependencies
- Use `uv sync` to install dependencies from `pyproject.toml`
- Use `uv add <package>` to add new dependencies
- Use `uv run <command>` to run commands in the project environment

Example workflow:
```bash
# Initialize project with uv
uv init

# Add dependencies
uv add pandas numpy xgboost

# Install all dependencies
uv sync

# Run scripts
uv run python scripts/download_data.py
```

### Version Control (GitHub)
- **Use GitHub CLI (`gh`)** for all GitHub operations
- Authenticate: `gh auth login`
- Create repos: `gh repo create`
- Create issues/PRs: `gh issue create`, `gh pr create`
- Clone repos: `gh repo clone <owner>/<repo>`

Example workflow:
```bash
# Initialize and push to GitHub
gh repo create fleet-cascade --private --source=. --remote=origin
git push -u origin main

# Create feature branch and PR
gh repo create-branch feature/forecasting
# ... make changes ...
gh pr create --title "Add demand forecasting" --body "Implements XGBoost forecasting"
```

### Data Sources (Kaggle)
- **Use Kaggle API** for downloading datasets
- Store Kaggle credentials in `~/.kaggle/kaggle.json`:
  ```json
  {
    "username": "your_username",
    "key": "your_api_key"
  }
  ```
- Use Kaggle CLI commands for dataset operations
- Download datasets to `data/raw/` directory

Example workflow:
```bash
# Install kaggle CLI (via uv)
uv add kaggle

# Download NYC Taxi dataset
kaggle datasets download -d <dataset-id> -p data/raw/nyc_taxi

# Download NASA Turbofan dataset
kaggle datasets download -d <dataset-id> -p data/raw/nasa_turbofan

# List available datasets
kaggle datasets list -s nyc
```

### Cloud Services (AWS)
- **Use AWS CLI (`aws`)** for all AWS operations
- Configure credentials: `aws configure` or use environment variables
- Store credentials securely (never commit to git)
- Use AWS profiles for multiple environments: `aws --profile prod s3 ls`

Example workflow:
```bash
# Configure AWS credentials
aws configure
# Enter: Access Key ID, Secret Access Key, Region, Output format

# S3 operations
aws s3 cp data/processed/forecasts.parquet s3://fleet-cascade-data/forecasts/
aws s3 sync data/models/ s3://fleet-cascade-models/

# SageMaker operations (Phase 4)
aws sagemaker create-training-job --training-job-name forecast-training-001 ...

# RDS operations
aws rds describe-db-instances
aws rds create-db-instance --db-instance-identifier fleet-db ...

# EC2/ECS operations
aws ecs list-clusters
aws ecs create-service --cluster fleet-cluster --service-name fleet-api ...

# Secrets Manager
aws secretsmanager get-secret-value --secret-id fleet-db-credentials
```

Python integration:
```python
import boto3

# S3 client
s3 = boto3.client('s3')
s3.download_file('fleet-cascade-data', 'forecasts.parquet', 'local.parquet')

# Secrets Manager
secrets = boto3.client('secretsmanager')
db_creds = secrets.get_secret_value(SecretId='fleet-db-credentials')
```

### Documentation (MkDocs)
- **Use MkDocs** for project documentation
- Create `docs/` directory with Markdown files
- Use `mkdocs.yml` for configuration
- Deploy to GitHub Pages or AWS S3/CloudFront

Setup:
```bash
# Install MkDocs
uv add mkdocs mkdocs-material

# Initialize (if starting fresh)
mkdocs new .

# Serve locally
mkdocs serve

# Build static site
mkdocs build

# Deploy to GitHub Pages
mkdocs gh-deploy
```

Recommended structure:
```
docs/
├── index.md              # Homepage
├── getting-started.md    # Setup instructions
├── architecture.md       # System architecture
├── api/                  # API documentation
│   ├── overview.md
│   ├── endpoints.md
│   └── examples.md
├── guides/              # How-to guides
│   ├── data-ingestion.md
│   ├── model-training.md
│   └── deployment.md
└── reference/           # Reference docs
    ├── configuration.md
    └── data-formats.md
```

Example `mkdocs.yml`:
```yaml
site_name: Fleet Decision Platform
site_description: Enterprise-grade decision intelligence platform for fleet operations

theme:
  name: material
  palette:
    - scheme: default
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    - scheme: slate
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  features:
    - navigation.tabs
    - navigation.sections
    - toc.integrate
    - search.suggest
    - content.code.annotate

nav:
  - Home: index.md
  - Getting Started: getting-started.md
  - Architecture: architecture.md
  - API:
    - Overview: api/overview.md
    - Endpoints: api/endpoints.md
    - Examples: api/examples.md
  - Guides:
    - Data Ingestion: guides/data-ingestion.md
    - Model Training: guides/model-training.md
    - Deployment: guides/deployment.md
  - Reference:
    - Configuration: reference/configuration.md
    - Data Formats: reference/data-formats.md

markdown_extensions:
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - pymdownx.superfences
  - admonition
  - tables
```

### Code Quality Tools
- **Use `ruff`** for linting and formatting (fast, modern, replaces black + flake8)
- Run `ruff check .` for linting
- Run `ruff format .` for formatting
- Add to pre-commit hooks (optional but recommended)

Configuration in `pyproject.toml`:
```toml
[tool.ruff]
line-length = 100
target-version = "py39"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W", "UP"]
ignore = []

[tool.ruff.format]
quote-style = "double"
```

### Environment Variables
- Use `.env` files for sensitive configuration (API keys, tokens)
- Never commit `.env` files (add to `.gitignore`)
- Use `python-dotenv` to load environment variables
- Create `.env.example` with placeholder values

Example:
```python
from dotenv import load_dotenv
import os

load_dotenv()
KAGGLE_USERNAME = os.getenv("KAGGLE_USERNAME")
KAGGLE_KEY = os.getenv("KAGGLE_KEY")
```

### Pre-commit Hooks (Recommended)
- Use `pre-commit` framework for code quality checks
- Install: `uv add --dev pre-commit`
- Create `.pre-commit-config.yaml`:
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.0
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
```

### Databases & Storage

#### PostgreSQL (Primary Database)
- **Use PostgreSQL** for structured data storage
- **Use `psql` CLI tool** for database operations and queries
- Store: Fleet state, allocation plans, forecasts, KPIs, historical data
- Use connection pooling (e.g., `psycopg2` or `asyncpg` for async)

Example workflow:
```bash
# Connect to database
psql -h localhost -U fleet_user -d fleet_db

# Common operations
psql -h localhost -U fleet_user -d fleet_db -c "SELECT * FROM forecasts LIMIT 10;"
psql -h localhost -U fleet_user -d fleet_db -f scripts/schema.sql
```

Python integration:
```python
# Use psycopg2 or asyncpg
import psycopg2
from psycopg2.extras import execute_values

conn = psycopg2.connect(
    host=os.getenv("POSTGRES_HOST"),
    database=os.getenv("POSTGRES_DB"),
    user=os.getenv("POSTGRES_USER"),
    password=os.getenv("POSTGRES_PASSWORD")
)
```

#### Pinecone (Vector Storage)
- **Use Pinecone** for vector storage and semantic search
- Primary use case: Contract intelligence (RAG-style semantic search over contracts)
- Store: Contract embeddings, constraint embeddings, document vectors
- Use for: Similarity search, contract querying, constraint retrieval

Example workflow:
```python
import pinecone
from pinecone import Pinecone

# Initialize
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("fleet-contracts")

# Store contract embeddings
index.upsert(vectors=[("contract_1", embedding_vector, metadata)])

# Semantic search
results = index.query(vector=query_embedding, top_k=5)
```

#### NoSQL Recommendations

**Redis** (Recommended for MVP+):
- **Use case**: Caching, session storage, real-time data
- Cache: API responses, model predictions, frequently accessed data
- Real-time: Current fleet state, active optimization runs
- Rate limiting: API rate limiting
- Message queue: Async task processing (optional)

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)
# Cache forecast
r.setex(f"forecast:{location_id}:{date}", 3600, forecast_json)
# Get cached forecast
cached = r.get(f"forecast:{location_id}:{date}")
```

**InfluxDB or TimescaleDB** (For time-series data):
- **Use case**: Time-series storage for demand forecasts, metrics, KPIs
- Better than PostgreSQL for: High-frequency time-series queries
- Store: Historical demand, forecast accuracy metrics, optimization KPIs over time
- **Recommendation**: Start with PostgreSQL + TimescaleDB extension (PostgreSQL-based), migrate to InfluxDB if needed

**MongoDB** (Optional, for flexible document storage):
- **Use case**: Store unstructured/semi-structured contract data before processing
- Store: Raw contract documents, extracted constraint JSON, flexible schemas
- **Recommendation**: Only if you need flexible document schemas; PostgreSQL JSONB might suffice

**SQLite** (For MVP/Development):
- **Use case**: Local development, testing, simple deployments
- **Recommendation**: Use for MVP, migrate to PostgreSQL for production
- No separate server needed, file-based

#### Storage Strategy by Phase

**Phase 1 (MVP)**:
- SQLite or PostgreSQL (local)
- Parquet files for data processing
- No vector storage needed (skip contract intelligence)

**Phase 2**:
- PostgreSQL for structured data
- Redis for caching
- Parquet files continue for data processing

**Phase 3** (Contract Intelligence):
- Add Pinecone for contract embeddings
- PostgreSQL for extracted constraints
- Redis for caching contract queries

**Phase 4** (Production):
- PostgreSQL (production instance)
- TimescaleDB extension for time-series
- Redis for caching and real-time data
- Pinecone for vector search
- Consider object storage (S3, GCS) for large files

### Docker (Phase 4)
- Use Docker for containerization
- Create `Dockerfile` in `docker/` directory
- Use `docker-compose.yml` for multi-container setup
- Include services: PostgreSQL, Redis (optional), application
- Build: `docker build -f docker/Dockerfile -t fleet-cascade .`
- Run: `docker-compose up`

Example `docker-compose.yml`:
```yaml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: fleet_db
      POSTGRES_USER: fleet_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  app:
    build: .
    depends_on:
      - postgres
      - redis
    environment:
      POSTGRES_HOST: postgres
      REDIS_HOST: redis
```

### Additional Recommended Tools

#### Data & ML Tools
- **DVC (Data Version Control)**: For versioning large data files (optional, Phase 2+)
- **Jupyter Lab**: For exploratory notebooks (install via `uv add jupyter`)
- **MLflow**: For experiment tracking and model versioning (Phase 2+)
- **Weights & Biases (wandb)**: Alternative to MLflow for experiment tracking (optional)

#### Development Tools
- **Makefile**: For common development tasks (download data, train models, run tests)
- **direnv**: For automatic environment variable loading (loads `.env` files automatically)
- **Task Runner**: Consider `invoke` or `nox` for complex build/test workflows (alternative to Makefile)

#### Monitoring & Observability (Phase 3+)
- **Prometheus**: For metrics collection
- **Grafana**: For visualization and dashboards
- **Sentry**: For error tracking and monitoring
- **Structured Logging**: Use `structlog` or `python-json-logger` for JSON logs

#### Task Queues & Async Processing (Phase 3+)
- **Celery**: For distributed task processing (with Redis/RabbitMQ backend)
- **RQ (Redis Queue)**: Simpler alternative to Celery, uses Redis
- **FastAPI BackgroundTasks**: For simple async tasks (built-in, no extra dependency)

#### API & Documentation
- **FastAPI built-in**: OpenAPI/Swagger docs automatically generated at `/docs` endpoint
- **MkDocs**: Project documentation (see Documentation section above)
- **Postman/Insomnia**: For API testing (optional)
- **HTTPie**: CLI tool for API testing (alternative to curl)

#### Secrets Management (Production)
- **Environment Variables**: Use `.env` files for development
- **HashiCorp Vault**: For production secrets management (optional)
- **AWS Secrets Manager / GCP Secret Manager**: Cloud-native secrets (if deploying to cloud)

#### Testing & Quality
- **pytest**: Testing framework (already mentioned)
- **pytest-cov**: Code coverage reporting
- **pytest-asyncio**: For testing async FastAPI endpoints
- **hypothesis**: Property-based testing (optional, for complex logic)

#### CI/CD (Phase 3+)
- **GitHub Actions**: For CI/CD pipelines
- **pre-commit**: Already mentioned for code quality hooks
- **Dependabot**: Automatic dependency updates (GitHub feature)

#### Cloud Deployment (Phase 4)
- **Primary Cloud**: AWS (use AWS CLI for all operations)
  - **SageMaker**: Model training and deployment
  - **ECS/Fargate**: Container orchestration for API
  - **RDS**: Managed PostgreSQL database
  - **S3**: Data storage, model artifacts, static assets
  - **Secrets Manager**: Secure credential storage
  - **CloudFront**: CDN for API and documentation
  - **ECR**: Container registry
  - Use AWS CLI for infrastructure management and deployments
- **Alternative Clouds**: GCP (Vertex AI, Cloud Run), Azure (ML Studio)
- **Simpler Options**: Fly.io / Railway for MVP deployments

## Code Style & Standards

### Python Conventions
- Follow PEP 8 style guide
- Use type hints for all function signatures (typing module)
- Maximum line length: 100 characters
- Use descriptive variable names (avoid abbreviations unless standard)
- Prefer f-strings for string formatting
- Use pathlib.Path for file paths, not os.path

### Imports Organization
```python
# Standard library
import logging
from pathlib import Path
from typing import Dict, List, Optional

# Third-party
import pandas as pd
import numpy as np
from xgboost import XGBRegressor

# Local imports
from src.utils.config import load_config
from src.forecasting.models.xgboost_model import XGBoostForecastModel
```

### Documentation
- All public functions/classes must have docstrings (Google style)
- Include type information in docstrings
- Document complex algorithms and optimization logic
- Add inline comments for non-obvious business logic

Example:
```python
def optimize_allocation(
    demand_forecast: pd.DataFrame,
    fleet_state: pd.DataFrame,
    network_costs: np.ndarray,
    constraints: Dict[str, float]
) -> pd.DataFrame:
    """
    Optimize fleet allocation using min-cost flow algorithm.

    Args:
        demand_forecast: DataFrame with columns [location, time, demand]
        fleet_state: DataFrame with columns [vehicle_id, location, capacity, status]
        network_costs: 2D array of zone-to-zone costs
        constraints: Dictionary of constraint parameters (max_distance, capacity, etc.)

    Returns:
        DataFrame with allocation plan: [vehicle_id, source_location, target_location, cost]

    Raises:
        OptimizationError: If solver fails or constraints are infeasible
    """
```

## Architecture Patterns

### Module Organization
- Follow the repository structure defined in plan.md
- Each major capability (forecasting, optimization, risk, contracts) is a separate module
- Keep modules loosely coupled with clear interfaces
- Use dependency injection for config and external dependencies

### Configuration Management
- **NEVER hardcode parameters** - always use config files (YAML/JSON)
- Load configs using `src.utils.config.load_config()`
- Pass config objects to functions, not individual parameters
- Support environment-specific configs (dev, prod, mvp)

Example:
```python
# GOOD
config = load_config("config/mvp_config.yaml")
model = XGBoostForecastModel(config.forecasting)

# BAD
model = XGBoostForecastModel(n_estimators=100, max_depth=5)  # Hardcoded!
```

### Data Handling
- Use Pandas DataFrames for tabular data
- Use Parquet format for persistence (efficient, columnar)
- Standardize column names: snake_case, descriptive
- Always validate data shapes and types before processing
- Use type hints: `pd.DataFrame`, `np.ndarray`, `Dict[str, Any]`

Data format conventions:
- Forecasts: `Dict[str, np.ndarray]` where key is location_id
- Risk scores: DataFrame with columns `[asset_id, risk_score, risk_category]`
- Allocation plans: DataFrame with columns `[vehicle_id, source_location, target_location, cost, assignment]`

### Error Handling
- Create custom exceptions per module:
  - `ForecastError` in forecasting module
  - `OptimizationError` in optimization module
  - `DataIngestionError` in data module
- Use structured logging (JSON format for production)
- Log at appropriate levels: DEBUG, INFO, WARNING, ERROR
- Include context in error messages (config values, data shapes, etc.)

Example:
```python
import logging

logger = logging.getLogger(__name__)

class OptimizationError(Exception):
    """Raised when optimization fails."""
    pass

def solve_min_cost_flow(...):
    try:
        # optimization logic
    except SolverException as e:
        logger.error(f"Optimization failed: {e}", extra={
            "num_vehicles": len(vehicles),
            "num_locations": len(locations),
            "solver": config.optimization.solver
        })
        raise OptimizationError(f"Solver failed: {e}") from e
```

## API Design (FastAPI)

### Endpoint Structure
- Use Pydantic models for all request/response validation
- Keep routes in `src/api/routes/` separate from business logic
- Use dependency injection for shared services (config, models)
- Return structured JSON responses with consistent format

Example:
```python
from pydantic import BaseModel
from fastapi import APIRouter, HTTPException

router = APIRouter(prefix="/api/v1", tags=["optimization"])

class OptimizationRequest(BaseModel):
    demand_forecast: Dict[str, List[float]]
    fleet_state: Dict[str, Any]
    constraints: Dict[str, float]

class OptimizationResponse(BaseModel):
    allocation_plan: List[Dict[str, Any]]
    total_cost: float
    kpis: Dict[str, float]
    status: str

@router.post("/optimize", response_model=OptimizationResponse)
async def optimize(request: OptimizationRequest):
    # Business logic here
    pass
```

### Response Format
Always return consistent JSON structure:
```json
{
  "status": "success" | "error",
  "data": {...},
  "metadata": {
    "timestamp": "...",
    "version": "..."
  },
  "errors": []  // if status is error
}
```

## Model Implementation Patterns

### Forecasting Models
- All models should implement a common interface/base class
- Models should be swappable (XGBoost, Prophet, TFT)
- Store model artifacts in `data/models/` with metadata
- Support both training and prediction modes

Example interface:
```python
from abc import ABC, abstractmethod

class ForecastModel(ABC):
    @abstractmethod
    def train(self, data: pd.DataFrame, config: Dict) -> None:
        """Train the model."""
        pass

    @abstractmethod
    def predict(self, features: pd.DataFrame) -> np.ndarray:
        """Generate forecasts."""
        pass

    @abstractmethod
    def save(self, path: Path) -> None:
        """Save model artifact."""
        pass
```

### Optimization Solvers
- Wrap OR-Tools/PuLP in abstraction layer
- Solvers should be swappable via config
- Return standardized allocation plan format
- Include solver status and diagnostics in output

## Testing Guidelines

### Test Structure
- Unit tests: Test individual functions in isolation
- Integration tests: Test end-to-end pipeline
- Use pytest as testing framework
- Place tests in `tests/` mirroring `src/` structure

### MVP Testing Approach
- Start with integration test for full pipeline
- Add unit tests incrementally as complexity grows
- Use fixtures for test data (small sample datasets)
- Mock external dependencies (data downloads, model training)

Example:
```python
import pytest
from src.optimization.cascade import CascadingOptimizer

def test_optimization_pipeline( sample_demand, sample_fleet, sample_config):
    """Integration test for optimization pipeline."""
    optimizer = CascadingOptimizer(sample_config)
    result = optimizer.optimize(sample_demand, sample_fleet)

    assert result.status == "success"
    assert result.total_cost > 0
    assert len(result.allocation_plan) > 0
```

## MVP-First Mindset

### Keep It Simple
- Start with minimal viable implementation
- Avoid over-engineering in Phase 1
- Use simple heuristics before complex ML models
- Hardcode constraints in config files before building contract extraction
- Single-stage optimization before cascading

### Progressive Enhancement
- Build working end-to-end flow first
- Add complexity incrementally
- Refactor when needed, not preemptively
- Document what's simplified for future phases

## Data Processing

### Ingestion
- Download and cache raw data (don't process on-the-fly)
- Store in `data/raw/` with clear naming
- Include data versioning/metadata

### Feature Engineering
- Keep feature engineering functions pure (no side effects)
- Document feature definitions
- Support feature selection via config

### Simulation
- Use random seeds for reproducibility
- Generate realistic but controlled synthetic data
- Document simulation parameters

## Performance Considerations

### Optimization
- Use vectorized operations (NumPy/Pandas) instead of loops
- Cache expensive computations (network costs, feature matrices)
- Profile before optimizing (measure first)

### API
- Use async/await for I/O operations
- Consider response caching for repeated queries
- Limit response payload sizes

## Code Review Checklist

Before submitting code, ensure:
- [ ] All parameters come from config, not hardcoded
- [ ] Type hints on all functions
- [ ] Docstrings on public functions
- [ ] Error handling with custom exceptions
- [ ] Logging at appropriate levels
- [ ] Data validation (shapes, types)
- [ ] Follows repository structure
- [ ] No circular imports
- [ ] Tests pass (if applicable)

## Common Patterns to Avoid

### Anti-patterns
- ❌ Hardcoding parameters in code
- ❌ Mixing business logic with API routes
- ❌ Using global variables for state
- ❌ Silent failures (catch and log, don't ignore)
- ❌ Tight coupling between modules
- ❌ Premature optimization

### Good Patterns
- ✅ Config-driven parameters
- ✅ Dependency injection
- ✅ Clear module boundaries
- ✅ Explicit error handling
- ✅ Type hints everywhere
- ✅ Logging for observability

## References

- See `plan.md` for architecture, data sources, and phased approach
- Repository structure: `plan.md` → Repository Structure section
- Tech stack: `plan.md` → Suggested Tech Stack section
- MVP simplifications: `plan.md` → Minimal Viable Product section
