{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöó Fleet Decision Platform - Complete Workflow\n",
        "\n",
        "> Enterprise-grade decision intelligence platform for fleet operations\n",
        "\n",
        "This notebook demonstrates the **end-to-end workflow** of the Fleet Decision Platform:\n",
        "\n",
        "1. **Data Loading & Exploration** - Load Uber rides and NASA turbofan datasets\n",
        "2. **Feature Engineering** - Create time-based and sensor-based features\n",
        "3. **Demand Forecasting** - XGBoost model for predicting ride demand\n",
        "4. **Risk Prediction** - Predict asset remaining useful life (RUL)\n",
        "5. **Fleet Simulation** - Generate synthetic fleet state\n",
        "6. **Optimization** - Min-cost flow optimization with OR-Tools\n",
        "7. **Explainability** - SHAP values and cost analysis\n",
        "8. **FastAPI Integration** - How to use the API\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.12.9)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/ashraf96/Desktop/Data-Driven-Fleet-Cascade/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Standard library\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Optimization\n",
        "from ortools.graph.python import min_cost_flow\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 50)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"üì¶ NumPy: {np.__version__}\")\n",
        "print(f\"üì¶ Pandas: {pd.__version__}\")\n",
        "print(f\"üì¶ XGBoost: {xgb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. üìä Data Loading & Exploration\n",
        "\n",
        "### 1.1 Load Uber Rides Data (Demand Forecasting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "DATA_DIR = Path(\"../data/raw\")\n",
        "UBER_PATH = DATA_DIR / \"uber_fares\" / \"uber.csv\"\n",
        "NASA_DIR = DATA_DIR / \"nasa_turbofan\" / \"CMaps\"\n",
        "\n",
        "# Load Uber data\n",
        "print(f\"üìÇ Loading Uber data from: {UBER_PATH}\")\n",
        "uber_df = pd.read_csv(UBER_PATH)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(uber_df):,} records\")\n",
        "print(f\"üìä Shape: {uber_df.shape}\")\n",
        "print(f\"üíæ Memory: {uber_df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
        "\n",
        "uber_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data info and statistics\n",
        "print(\"üìã Data Types:\")\n",
        "print(uber_df.dtypes)\n",
        "print(\"\\nüìà Statistics:\")\n",
        "uber_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"üîç Missing Values:\")\n",
        "missing = uber_df.isnull().sum()\n",
        "missing_pct = (missing / len(uber_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
        "print(missing_df[missing_df['Missing'] > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Load NASA Turbofan Data (Risk Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NASA C-MAPSS Dataset columns\n",
        "# Columns: unit_id, time_cycles, op_setting1-3, sensor1-21\n",
        "sensor_columns = ['sensor_' + str(i) for i in range(1, 22)]\n",
        "op_columns = ['op_setting_' + str(i) for i in range(1, 4)]\n",
        "column_names = ['unit_id', 'time_cycles'] + op_columns + sensor_columns\n",
        "\n",
        "# Load training data (FD001 - simplest subset)\n",
        "train_fd001 = pd.read_csv(\n",
        "    NASA_DIR / \"train_FD001.txt\",\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    names=column_names\n",
        ")\n",
        "\n",
        "# Load RUL (Remaining Useful Life) labels\n",
        "rul_fd001 = pd.read_csv(NASA_DIR / \"RUL_FD001.txt\", header=None, names=['RUL'])\n",
        "\n",
        "print(f\"‚úÖ NASA Turbofan FD001 loaded\")\n",
        "print(f\"üìä Train shape: {train_fd001.shape}\")\n",
        "print(f\"üîß Unique engines: {train_fd001['unit_id'].nunique()}\")\n",
        "print(f\"üìà Total cycles: {len(train_fd001):,}\")\n",
        "\n",
        "train_fd001.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. üîß Data Preprocessing & Feature Engineering\n",
        "\n",
        "### 2.1 Uber Data - Time-based Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean and process Uber data\n",
        "uber_clean = uber_df.copy()\n",
        "\n",
        "# Drop rows with missing values in key columns\n",
        "uber_clean = uber_clean.dropna(subset=['pickup_datetime', 'fare_amount', 'pickup_longitude', 'pickup_latitude'])\n",
        "\n",
        "# Parse datetime\n",
        "uber_clean['pickup_datetime'] = pd.to_datetime(uber_clean['pickup_datetime'], errors='coerce')\n",
        "uber_clean = uber_clean.dropna(subset=['pickup_datetime'])\n",
        "\n",
        "# Extract time features\n",
        "uber_clean['hour'] = uber_clean['pickup_datetime'].dt.hour\n",
        "uber_clean['day_of_week'] = uber_clean['pickup_datetime'].dt.dayofweek\n",
        "uber_clean['month'] = uber_clean['pickup_datetime'].dt.month\n",
        "uber_clean['year'] = uber_clean['pickup_datetime'].dt.year\n",
        "uber_clean['is_weekend'] = uber_clean['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Create time period bins\n",
        "def get_time_period(hour):\n",
        "    if 6 <= hour < 12:\n",
        "        return 'morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'evening'\n",
        "    else:\n",
        "        return 'night'\n",
        "\n",
        "uber_clean['time_period'] = uber_clean['hour'].apply(get_time_period)\n",
        "\n",
        "# Filter reasonable fare amounts and coordinates\n",
        "uber_clean = uber_clean[\n",
        "    (uber_clean['fare_amount'] > 0) & \n",
        "    (uber_clean['fare_amount'] < 500) &\n",
        "    (uber_clean['pickup_longitude'].between(-75, -73)) &\n",
        "    (uber_clean['pickup_latitude'].between(40, 42))\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Cleaned data: {len(uber_clean):,} records ({len(uber_clean)/len(uber_df)*100:.1f}% retained)\")\n",
        "uber_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create location zones using grid-based clustering\n",
        "# Divide NYC into zones based on coordinates\n",
        "def create_zone(lon, lat, n_zones=5):\n",
        "    \"\"\"Create zone ID based on longitude/latitude grid\"\"\"\n",
        "    lon_bins = np.linspace(-74.05, -73.75, n_zones + 1)\n",
        "    lat_bins = np.linspace(40.6, 40.9, n_zones + 1)\n",
        "    \n",
        "    lon_zone = np.digitize(lon, lon_bins) - 1\n",
        "    lat_zone = np.digitize(lat, lat_bins) - 1\n",
        "    \n",
        "    # Clip to valid range\n",
        "    lon_zone = np.clip(lon_zone, 0, n_zones - 1)\n",
        "    lat_zone = np.clip(lat_zone, 0, n_zones - 1)\n",
        "    \n",
        "    return lat_zone * n_zones + lon_zone\n",
        "\n",
        "uber_clean['zone_id'] = create_zone(\n",
        "    uber_clean['pickup_longitude'].values,\n",
        "    uber_clean['pickup_latitude'].values\n",
        ")\n",
        "\n",
        "print(f\"üìç Created {uber_clean['zone_id'].nunique()} zones\")\n",
        "uber_clean['zone_id'].value_counts().head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate demand by hour and zone\n",
        "uber_clean['date_hour'] = uber_clean['pickup_datetime'].dt.floor('h')\n",
        "\n",
        "demand_df = uber_clean.groupby(['date_hour', 'zone_id']).agg(\n",
        "    demand=('fare_amount', 'count'),\n",
        "    avg_fare=('fare_amount', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Add time features to aggregated data\n",
        "demand_df['hour'] = demand_df['date_hour'].dt.hour\n",
        "demand_df['day_of_week'] = demand_df['date_hour'].dt.dayofweek\n",
        "demand_df['month'] = demand_df['date_hour'].dt.month\n",
        "demand_df['is_weekend'] = demand_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "print(f\"‚úÖ Aggregated demand data: {len(demand_df):,} records\")\n",
        "print(f\"üìä Date range: {demand_df['date_hour'].min()} to {demand_df['date_hour'].max()}\")\n",
        "demand_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 NASA Turbofan - RUL Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate RUL (Remaining Useful Life) for training data\n",
        "# RUL = max_cycles - current_cycle for each engine\n",
        "\n",
        "def add_rul(df):\n",
        "    \"\"\"Add RUL column to turbofan data\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Get max cycle for each engine\n",
        "    max_cycles = df.groupby('unit_id')['time_cycles'].max().reset_index()\n",
        "    max_cycles.columns = ['unit_id', 'max_cycle']\n",
        "    \n",
        "    # Merge and calculate RUL\n",
        "    df = df.merge(max_cycles, on='unit_id')\n",
        "    df['RUL'] = df['max_cycle'] - df['time_cycles']\n",
        "    df = df.drop('max_cycle', axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "train_fd001_rul = add_rul(train_fd001)\n",
        "\n",
        "print(f\"‚úÖ Added RUL to training data\")\n",
        "print(f\"üìä RUL range: {train_fd001_rul['RUL'].min()} - {train_fd001_rul['RUL'].max()}\")\n",
        "train_fd001_rul.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. üìà Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize demand patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Demand by hour of day\n",
        "hourly_demand = demand_df.groupby('hour')['demand'].mean()\n",
        "axes[0, 0].bar(hourly_demand.index, hourly_demand.values, color='steelblue', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Hour of Day')\n",
        "axes[0, 0].set_ylabel('Average Demand')\n",
        "axes[0, 0].set_title('üïê Demand by Hour of Day')\n",
        "axes[0, 0].set_xticks(range(0, 24, 2))\n",
        "\n",
        "# 2. Demand by day of week\n",
        "dow_demand = demand_df.groupby('day_of_week')['demand'].mean()\n",
        "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "axes[0, 1].bar(range(7), dow_demand.values, color='coral', alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Day of Week')\n",
        "axes[0, 1].set_ylabel('Average Demand')\n",
        "axes[0, 1].set_title('üìÖ Demand by Day of Week')\n",
        "axes[0, 1].set_xticks(range(7))\n",
        "axes[0, 1].set_xticklabels(days)\n",
        "\n",
        "# 3. Demand distribution\n",
        "axes[1, 0].hist(demand_df['demand'], bins=50, color='green', alpha=0.7, edgecolor='white')\n",
        "axes[1, 0].set_xlabel('Demand (trips per hour)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('üìä Demand Distribution')\n",
        "\n",
        "# 4. Demand heatmap by hour and day\n",
        "pivot_demand = demand_df.pivot_table(\n",
        "    values='demand', \n",
        "    index='day_of_week', \n",
        "    columns='hour', \n",
        "    aggfunc='mean'\n",
        ")\n",
        "sns.heatmap(pivot_demand, cmap='YlOrRd', ax=axes[1, 1], cbar_kws={'label': 'Avg Demand'})\n",
        "axes[1, 1].set_xlabel('Hour of Day')\n",
        "axes[1, 1].set_ylabel('Day of Week')\n",
        "axes[1, 1].set_yticklabels(days)\n",
        "axes[1, 1].set_title('üî• Demand Heatmap')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/outputs/demand_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved visualization to data/outputs/demand_analysis.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize turbofan sensor degradation\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Sample engine\n",
        "sample_engine = train_fd001_rul[train_fd001_rul['unit_id'] == 1]\n",
        "\n",
        "# 1. RUL over time for sample engine\n",
        "axes[0, 0].plot(sample_engine['time_cycles'], sample_engine['RUL'], 'b-', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Time Cycles')\n",
        "axes[0, 0].set_ylabel('Remaining Useful Life')\n",
        "axes[0, 0].set_title('‚è≥ RUL Degradation (Engine 1)')\n",
        "axes[0, 0].fill_between(sample_engine['time_cycles'], sample_engine['RUL'], alpha=0.3)\n",
        "\n",
        "# 2. Sensor 2 degradation (one of the most indicative sensors)\n",
        "for engine_id in [1, 5, 10, 15]:\n",
        "    engine_data = train_fd001_rul[train_fd001_rul['unit_id'] == engine_id]\n",
        "    axes[0, 1].plot(engine_data['time_cycles'], engine_data['sensor_2'], alpha=0.7, label=f'Engine {engine_id}')\n",
        "axes[0, 1].set_xlabel('Time Cycles')\n",
        "axes[0, 1].set_ylabel('Sensor 2 Value')\n",
        "axes[0, 1].set_title('üìâ Sensor 2 Degradation Pattern')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. RUL distribution\n",
        "axes[1, 0].hist(train_fd001_rul['RUL'], bins=50, color='purple', alpha=0.7, edgecolor='white')\n",
        "axes[1, 0].set_xlabel('RUL (cycles)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('üìä RUL Distribution')\n",
        "\n",
        "# 4. Correlation heatmap of key sensors with RUL\n",
        "key_sensors = ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_11', 'sensor_12', 'RUL']\n",
        "corr_matrix = train_fd001_rul[key_sensors].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1], fmt='.2f')\n",
        "axes[1, 1].set_title('üîó Sensor-RUL Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/outputs/turbofan_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Saved visualization to data/outputs/turbofan_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. ü§ñ Demand Forecasting with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for demand forecasting\n",
        "feature_cols = ['hour', 'day_of_week', 'month', 'is_weekend', 'zone_id']\n",
        "target_col = 'demand'\n",
        "\n",
        "# Create lagged features (previous hour demand)\n",
        "demand_df_sorted = demand_df.sort_values(['zone_id', 'date_hour'])\n",
        "demand_df_sorted['demand_lag_1'] = demand_df_sorted.groupby('zone_id')['demand'].shift(1)\n",
        "demand_df_sorted['demand_lag_24'] = demand_df_sorted.groupby('zone_id')['demand'].shift(24)  # Same hour yesterday\n",
        "demand_df_sorted = demand_df_sorted.dropna()\n",
        "\n",
        "# Add lag features to feature list\n",
        "feature_cols_extended = feature_cols + ['demand_lag_1', 'demand_lag_24']\n",
        "\n",
        "X = demand_df_sorted[feature_cols_extended]\n",
        "y = demand_df_sorted[target_col]\n",
        "\n",
        "# Train-test split (time-based)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"üìä Training samples: {len(X_train):,}\")\n",
        "print(f\"üìä Test samples: {len(X_test):,}\")\n",
        "print(f\"üìã Features: {feature_cols_extended}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model for demand forecasting\n",
        "demand_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training XGBoost demand model...\")\n",
        "demand_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_pred = demand_model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n‚úÖ Model Training Complete!\")\n",
        "print(f\"üìà RMSE: {rmse:.2f}\")\n",
        "print(f\"üìà MAE: {mae:.2f}\")\n",
        "print(f\"üìà R¬≤ Score: {r2:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols_extended,\n",
        "    'importance': demand_model.feature_importances_\n",
        "}).sort_values('importance', ascending=True)\n",
        "\n",
        "axes[0].barh(feature_importance['feature'], feature_importance['importance'], color='steelblue')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('üéØ Feature Importance (Demand Model)')\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[1].scatter(y_test, y_pred, alpha=0.3, s=10)\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "axes[1].set_xlabel('Actual Demand')\n",
        "axes[1].set_ylabel('Predicted Demand')\n",
        "axes[1].set_title(f'üìä Actual vs Predicted (R¬≤ = {r2:.3f})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/outputs/demand_model_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. ‚ö†Ô∏è Risk Prediction (RUL Forecasting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for RUL prediction\n",
        "# Use sensor readings and operational settings\n",
        "\n",
        "# Select relevant sensors (based on correlation analysis)\n",
        "rul_features = ['time_cycles'] + op_columns + ['sensor_2', 'sensor_3', 'sensor_4', \n",
        "                                                'sensor_7', 'sensor_11', 'sensor_12', \n",
        "                                                'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
        "\n",
        "X_rul = train_fd001_rul[rul_features]\n",
        "y_rul = train_fd001_rul['RUL']\n",
        "\n",
        "# Clip RUL to max 125 (piece-wise linear assumption)\n",
        "y_rul_clipped = y_rul.clip(upper=125)\n",
        "\n",
        "# Train-test split\n",
        "X_rul_train, X_rul_test, y_rul_train, y_rul_test = train_test_split(\n",
        "    X_rul, y_rul_clipped, test_size=0.2, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_rul_train_scaled = scaler.fit_transform(X_rul_train)\n",
        "X_rul_test_scaled = scaler.transform(X_rul_test)\n",
        "\n",
        "print(f\"üìä RUL Training samples: {len(X_rul_train):,}\")\n",
        "print(f\"üìä RUL Test samples: {len(X_rul_test):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model for RUL prediction\n",
        "rul_model = xgb.XGBRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training XGBoost RUL model...\")\n",
        "rul_model.fit(X_rul_train_scaled, y_rul_train, verbose=False)\n",
        "\n",
        "# Predictions\n",
        "y_rul_pred = rul_model.predict(X_rul_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "rul_rmse = np.sqrt(mean_squared_error(y_rul_test, y_rul_pred))\n",
        "rul_mae = mean_absolute_error(y_rul_test, y_rul_pred)\n",
        "rul_r2 = r2_score(y_rul_test, y_rul_pred)\n",
        "\n",
        "print(f\"\\n‚úÖ RUL Model Training Complete!\")\n",
        "print(f\"üìà RMSE: {rul_rmse:.2f} cycles\")\n",
        "print(f\"üìà MAE: {rul_mae:.2f} cycles\")\n",
        "print(f\"üìà R¬≤ Score: {rul_r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. üöó Fleet Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simulated fleet state\n",
        "NUM_VEHICLES = 50\n",
        "NUM_ZONES = 25  # 5x5 grid\n",
        "\n",
        "def generate_fleet_state(n_vehicles, n_zones, seed=42):\n",
        "    \"\"\"Generate simulated fleet state\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    fleet = pd.DataFrame({\n",
        "        'vehicle_id': [f'V{i:03d}' for i in range(1, n_vehicles + 1)],\n",
        "        'current_zone': np.random.randint(0, n_zones, n_vehicles),\n",
        "        'capacity': np.ones(n_vehicles, dtype=int),\n",
        "        'status': np.random.choice(['operational', 'operational', 'operational', 'maintenance'], n_vehicles),\n",
        "        'mileage_km': np.random.randint(10000, 100000, n_vehicles),\n",
        "        'age_months': np.random.randint(6, 60, n_vehicles),\n",
        "        'risk_score': np.random.uniform(0.1, 0.9, n_vehicles)\n",
        "    })\n",
        "    \n",
        "    return fleet\n",
        "\n",
        "fleet_state = generate_fleet_state(NUM_VEHICLES, NUM_ZONES)\n",
        "operational_fleet = fleet_state[fleet_state['status'] == 'operational'].copy()\n",
        "\n",
        "print(f\"üöó Fleet State Generated:\")\n",
        "print(f\"   Total vehicles: {len(fleet_state)}\")\n",
        "print(f\"   Operational: {len(operational_fleet)}\")\n",
        "print(f\"   In maintenance: {len(fleet_state) - len(operational_fleet)}\")\n",
        "\n",
        "fleet_state.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate network cost matrix (zone-to-zone travel costs)\n",
        "def generate_network_costs(n_zones, seed=42):\n",
        "    \"\"\"Generate zone-to-zone travel cost matrix\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Create grid positions\n",
        "    grid_size = int(np.sqrt(n_zones))\n",
        "    positions = [(i // grid_size, i % grid_size) for i in range(n_zones)]\n",
        "    \n",
        "    # Calculate Euclidean distances and scale to costs\n",
        "    costs = np.zeros((n_zones, n_zones))\n",
        "    for i in range(n_zones):\n",
        "        for j in range(n_zones):\n",
        "            dist = np.sqrt((positions[i][0] - positions[j][0])**2 + \n",
        "                          (positions[i][1] - positions[j][1])**2)\n",
        "            # Cost = distance * base_rate + random_factor\n",
        "            costs[i, j] = dist * 5 + np.random.uniform(0, 2)\n",
        "    \n",
        "    return costs\n",
        "\n",
        "network_costs = generate_network_costs(NUM_ZONES)\n",
        "\n",
        "print(f\"üìä Network Cost Matrix: {network_costs.shape}\")\n",
        "print(f\"   Min cost: ${network_costs[network_costs > 0].min():.2f}\")\n",
        "print(f\"   Max cost: ${network_costs.max():.2f}\")\n",
        "print(f\"   Avg cost: ${network_costs[network_costs > 0].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. ‚ö° Fleet Optimization with OR-Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate demand forecast for optimization (use predicted demand per zone)\n",
        "# For demo, use simulated demand based on patterns learned\n",
        "\n",
        "def generate_demand_forecast(n_zones, hour=18, day_of_week=4, seed=42):\n",
        "    \"\"\"Generate demand forecast per zone\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Base demand varies by zone (center zones have higher demand)\n",
        "    grid_size = int(np.sqrt(n_zones))\n",
        "    base_demand = np.zeros(n_zones)\n",
        "    \n",
        "    for z in range(n_zones):\n",
        "        row, col = z // grid_size, z % grid_size\n",
        "        # Distance from center\n",
        "        center_dist = np.sqrt((row - grid_size/2)**2 + (col - grid_size/2)**2)\n",
        "        # Higher demand near center\n",
        "        base_demand[z] = max(5, 15 - center_dist * 2) + np.random.randint(0, 5)\n",
        "    \n",
        "    # Adjust for time of day (peak hours)\n",
        "    if 17 <= hour <= 19:  # Evening rush\n",
        "        time_multiplier = 1.5\n",
        "    elif 7 <= hour <= 9:   # Morning rush  \n",
        "        time_multiplier = 1.3\n",
        "    else:\n",
        "        time_multiplier = 1.0\n",
        "    \n",
        "    # Weekend adjustment\n",
        "    if day_of_week >= 5:\n",
        "        time_multiplier *= 0.8\n",
        "    \n",
        "    return (base_demand * time_multiplier).astype(int)\n",
        "\n",
        "demand_forecast = generate_demand_forecast(NUM_ZONES, hour=18, day_of_week=4)\n",
        "\n",
        "print(f\"üìä Demand Forecast Generated:\")\n",
        "print(f\"   Total demand: {demand_forecast.sum()} trips\")\n",
        "print(f\"   Zones with demand: {(demand_forecast > 0).sum()}\")\n",
        "print(f\"   Max zone demand: {demand_forecast.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Min-Cost Flow Optimization using OR-Tools\n",
        "def optimize_fleet_allocation(fleet_df, demand, costs, max_cost_per_vehicle=50):\n",
        "    \"\"\"\n",
        "    Optimize fleet allocation using min-cost flow\n",
        "    \n",
        "    Network structure:\n",
        "    - Source node (0): supplies all vehicles\n",
        "    - Vehicle nodes (1 to n_vehicles): one per vehicle\n",
        "    - Zone nodes (n_vehicles+1 to n_vehicles+n_zones): one per zone\n",
        "    - Sink node (last): absorbs satisfied demand\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get operational vehicles\n",
        "    op_fleet = fleet_df[fleet_df['status'] == 'operational'].copy()\n",
        "    n_vehicles = len(op_fleet)\n",
        "    n_zones = len(demand)\n",
        "    \n",
        "    # Node indices\n",
        "    SOURCE = 0\n",
        "    vehicle_nodes = list(range(1, n_vehicles + 1))\n",
        "    zone_nodes = list(range(n_vehicles + 1, n_vehicles + 1 + n_zones))\n",
        "    SINK = n_vehicles + 1 + n_zones\n",
        "    \n",
        "    # Create the min cost flow solver\n",
        "    smcf = min_cost_flow.SimpleMinCostFlow()\n",
        "    \n",
        "    # Add arcs from source to each vehicle (capacity=1, cost=0)\n",
        "    for i, v_node in enumerate(vehicle_nodes):\n",
        "        smcf.add_arc_with_capacity_and_unit_cost(SOURCE, v_node, 1, 0)\n",
        "    \n",
        "    # Add arcs from each vehicle to each zone (based on travel cost)\n",
        "    vehicle_zones = op_fleet['current_zone'].values\n",
        "    for i, (v_node, v_zone) in enumerate(zip(vehicle_nodes, vehicle_zones)):\n",
        "        for j, z_node in enumerate(zone_nodes):\n",
        "            zone_idx = j\n",
        "            travel_cost = int(costs[v_zone, zone_idx] * 100)  # Scale to int\n",
        "            if travel_cost < max_cost_per_vehicle * 100:  # Only add if within budget\n",
        "                smcf.add_arc_with_capacity_and_unit_cost(v_node, z_node, 1, travel_cost)\n",
        "    \n",
        "    # Add arcs from each zone to sink (capacity = demand)\n",
        "    for j, z_node in enumerate(zone_nodes):\n",
        "        zone_demand = min(int(demand[j]), n_vehicles)  # Cap at available vehicles\n",
        "        smcf.add_arc_with_capacity_and_unit_cost(z_node, SINK, zone_demand, 0)\n",
        "    \n",
        "    # Set supplies: source supplies all vehicles, sink demands minimum of total demand or vehicles\n",
        "    total_supply = n_vehicles\n",
        "    total_demand = min(int(demand.sum()), n_vehicles)\n",
        "    \n",
        "    smcf.set_node_supply(SOURCE, total_supply)\n",
        "    smcf.set_node_supply(SINK, -total_demand)\n",
        "    \n",
        "    # Solve\n",
        "    status = smcf.solve()\n",
        "    \n",
        "    results = {\n",
        "        'status': 'optimal' if status == smcf.OPTIMAL else 'infeasible',\n",
        "        'total_cost': 0,\n",
        "        'allocations': [],\n",
        "        'coverage': 0\n",
        "    }\n",
        "    \n",
        "    if status == smcf.OPTIMAL:\n",
        "        results['total_cost'] = smcf.optimal_cost() / 100  # Unscale\n",
        "        \n",
        "        # Extract allocations\n",
        "        for arc in range(smcf.num_arcs()):\n",
        "            if smcf.flow(arc) > 0:\n",
        "                tail = smcf.tail(arc)\n",
        "                head = smcf.head(arc)\n",
        "                \n",
        "                # Vehicle to zone assignment\n",
        "                if tail in vehicle_nodes and head in zone_nodes:\n",
        "                    v_idx = tail - 1\n",
        "                    z_idx = head - n_vehicles - 1\n",
        "                    results['allocations'].append({\n",
        "                        'vehicle_id': op_fleet.iloc[v_idx]['vehicle_id'],\n",
        "                        'from_zone': int(vehicle_zones[v_idx]),\n",
        "                        'to_zone': z_idx,\n",
        "                        'cost': costs[vehicle_zones[v_idx], z_idx]\n",
        "                    })\n",
        "        \n",
        "        # Calculate coverage\n",
        "        zones_served = len(set(a['to_zone'] for a in results['allocations']))\n",
        "        results['coverage'] = zones_served / n_zones\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run optimization\n",
        "print(\"üöÄ Running Min-Cost Flow Optimization...\")\n",
        "opt_results = optimize_fleet_allocation(fleet_state, demand_forecast, network_costs)\n",
        "\n",
        "print(f\"\\n‚úÖ Optimization Complete!\")\n",
        "print(f\"   Status: {opt_results['status']}\")\n",
        "print(f\"   Total Cost: ${opt_results['total_cost']:.2f}\")\n",
        "print(f\"   Vehicles Allocated: {len(opt_results['allocations'])}\")\n",
        "print(f\"   Zone Coverage: {opt_results['coverage']*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize allocation results\n",
        "allocation_df = pd.DataFrame(opt_results['allocations'])\n",
        "\n",
        "if len(allocation_df) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # 1. Vehicles per target zone\n",
        "    zone_counts = allocation_df['to_zone'].value_counts().sort_index()\n",
        "    axes[0].bar(zone_counts.index, zone_counts.values, color='steelblue', alpha=0.7)\n",
        "    axes[0].set_xlabel('Zone ID')\n",
        "    axes[0].set_ylabel('Vehicles Allocated')\n",
        "    axes[0].set_title('üöó Vehicle Allocation by Zone')\n",
        "    \n",
        "    # 2. Cost distribution\n",
        "    axes[1].hist(allocation_df['cost'], bins=15, color='coral', alpha=0.7, edgecolor='white')\n",
        "    axes[1].set_xlabel('Travel Cost ($)')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title('üí∞ Allocation Cost Distribution')\n",
        "    axes[1].axvline(allocation_df['cost'].mean(), color='red', linestyle='--', \n",
        "                    label=f'Mean: ${allocation_df[\"cost\"].mean():.2f}')\n",
        "    axes[1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../data/outputs/optimization_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüìã Sample Allocations:\")\n",
        "    print(allocation_df.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"‚ùå No allocations made\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. üåê FastAPI Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FastAPI endpoint example (for reference)\n",
        "# Run with: uvicorn src.api.main:app --reload\n",
        "\n",
        "fastapi_example = '''\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "app = FastAPI(title=\"Fleet Decision Platform\")\n",
        "\n",
        "class OptimizationRequest(BaseModel):\n",
        "    demand_forecast: Dict[str, List[float]]\n",
        "    fleet_state: Dict[str, Any]\n",
        "    constraints: Dict[str, float] = {}\n",
        "\n",
        "class OptimizationResponse(BaseModel):\n",
        "    status: str\n",
        "    allocation_plan: List[Dict[str, Any]]\n",
        "    total_cost: float\n",
        "    kpis: Dict[str, float]\n",
        "\n",
        "@app.post(\"/api/v1/optimize\", response_model=OptimizationResponse)\n",
        "async def optimize(request: OptimizationRequest):\n",
        "    \"\"\"Run fleet optimization\"\"\"\n",
        "    # 1. Load demand forecast\n",
        "    # 2. Get current fleet state  \n",
        "    # 3. Run optimization\n",
        "    # 4. Return allocation plan\n",
        "    \n",
        "    result = optimize_fleet_allocation(\n",
        "        fleet_df=...,\n",
        "        demand=...,\n",
        "        costs=...\n",
        "    )\n",
        "    \n",
        "    return OptimizationResponse(\n",
        "        status=result['status'],\n",
        "        allocation_plan=result['allocations'],\n",
        "        total_cost=result['total_cost'],\n",
        "        kpis={\n",
        "            'coverage': result['coverage'],\n",
        "            'vehicles_allocated': len(result['allocations'])\n",
        "        }\n",
        "    )\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\"}\n",
        "'''\n",
        "\n",
        "print(\"üìù FastAPI Example Code:\")\n",
        "print(fastapi_example)\n",
        "\n",
        "print(\"\\nüöÄ To start the API server:\")\n",
        "print(\"   uvicorn src.api.main:app --reload --port 8000\")\n",
        "print(\"\\nüìñ API Documentation available at:\")\n",
        "print(\"   http://localhost:8000/docs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Summary & KPIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\" * 60)\n",
        "print(\"üöó FLEET DECISION PLATFORM - WORKFLOW SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä DATA LOADED:\")\n",
        "print(f\"   ‚Ä¢ Uber Rides: {len(uber_df):,} records\")\n",
        "print(f\"   ‚Ä¢ NASA Turbofan: {len(train_fd001):,} sensor readings\")\n",
        "print(f\"   ‚Ä¢ Fleet Size: {NUM_VEHICLES} vehicles\")\n",
        "print(f\"   ‚Ä¢ Service Zones: {NUM_ZONES} zones\")\n",
        "\n",
        "print(\"\\nü§ñ DEMAND FORECASTING MODEL:\")\n",
        "print(f\"   ‚Ä¢ Algorithm: XGBoost Regressor\")\n",
        "print(f\"   ‚Ä¢ Features: {len(feature_cols_extended)}\")\n",
        "print(f\"   ‚Ä¢ RMSE: {rmse:.2f}\")\n",
        "print(f\"   ‚Ä¢ MAE: {mae:.2f}\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Score: {r2:.3f}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è RISK PREDICTION MODEL (RUL):\")\n",
        "print(f\"   ‚Ä¢ Algorithm: XGBoost Regressor\")\n",
        "print(f\"   ‚Ä¢ Features: {len(rul_features)}\")\n",
        "print(f\"   ‚Ä¢ RMSE: {rul_rmse:.2f} cycles\")\n",
        "print(f\"   ‚Ä¢ MAE: {rul_mae:.2f} cycles\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Score: {rul_r2:.3f}\")\n",
        "\n",
        "print(\"\\n‚ö° OPTIMIZATION RESULTS:\")\n",
        "print(f\"   ‚Ä¢ Solver: OR-Tools Min-Cost Flow\")\n",
        "print(f\"   ‚Ä¢ Status: {opt_results['status'].upper()}\")\n",
        "print(f\"   ‚Ä¢ Total Rebalancing Cost: ${opt_results['total_cost']:.2f}\")\n",
        "print(f\"   ‚Ä¢ Vehicles Allocated: {len(opt_results['allocations'])}\")\n",
        "print(f\"   ‚Ä¢ Zone Coverage: {opt_results['coverage']*100:.1f}%\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUTS SAVED:\")\n",
        "print(\"   ‚Ä¢ data/outputs/demand_analysis.png\")\n",
        "print(\"   ‚Ä¢ data/outputs/turbofan_analysis.png\")\n",
        "print(\"   ‚Ä¢ data/outputs/demand_model_results.png\")\n",
        "print(\"   ‚Ä¢ data/outputs/optimization_results.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ WORKFLOW COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
